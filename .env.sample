# Okta tenant details
OKTA_CLIENT_ORGURL=https://dev-1602.okta.com

# Replace with your Okta API token
# You can generate an API token in Okta by going to Admin > Security > API > Tokens
# Read only permissions are fine currently
OKTA_API_TOKEN=

# OAuth Client Configuration (required for OAuth proxy server)
# Create an OAuth application in Okta Admin Console > Applications > Create App Integration
# Choose "Web Application" and note the Client ID and Secret
OKTA_CLIENT_ID=your-oauth-client-id
OKTA_CLIENT_SECRET=your-oauth-client-secret
OKTA_ORG_URL=https://dev-1602.okta.com

# OAuth Security Settings
# Audience for JWT tokens - should match your application identifier
OKTA_OAUTH_AUDIENCE=fctrid-okta-mcp-server
# Callback URL - must match what's registered in your Okta OAuth app
OAUTH_REDIRECT_URI=http://localhost:3001/oauth/callback
# Require HTTPS for OAuth (set to false for local development)
OAUTH_REQUIRE_HTTPS=false

# Optional: Custom OAuth scopes (space-separated)
# Default: "openid profile email okta.users.read okta.groups.read okta.apps.read okta.events.read okta.logs.read okta.policies.read okta.devices.read okta.factors.read"
#OAUTH_SCOPES=openid profile email okta.users.read

# Optional: Session security (for production deployments)
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
#SESSION_SECRET_KEY=your-base64-encoded-32-byte-key

#additional settings
LOG_LEVEL=INFO


### Do NOT change this for now ####
OKTA_CONCURRENT_LIMIT=15

# AI Provider Selection. It can be one of the following values: 
# vertex_ai, openai, azure_openai, openai_compatible, anthropic
AI_PROVIDER=openai_compatible

# Vertex AI Models (if using Vertex AI)
## or set GOOGLE_APPLICATION_CREDENTIALS environment variable and comment this line
VERTEX_AI_SERVICE_ACCOUNT_FILE=path/to/service-account.json
VERTEX_AI_REASONING_MODEL=gemini-2.5-pro-exp-03-25

# OpenAI Models (if using OpenAI)
OPENAI_API_KEY=
OPENAI_CODING_MODEL=gpt-4o

# Azure OpenAI Models (if using Azure)
AZURE_OPENAI_KEY=your-api-key
AZURE_OPENAI_ENDPOINT=your-endpoint
AZURE_OPENAI_VERSION=2024-07-01-preview
AZURE_OPENAI_REASONING_MODEL=gpt-4


# OpenAI Compatible Configuration
OPENAI_COMPATIBLE_REASONING_MODEL=accounts/fireworks/models/deepseek-v3
OPENAI_COMPATIBLE_BASE_URL=https://api.fireworks.ai/inference/v1
OPENAI_COMPATIBLE_TOKEN=

# CUSTOM LLM / HTTP PROXY headers for openai_compatible model (if needed)
CUSTOM_HTTP_HEADERS={"x-ai-organization": "org-0a1b2c3d49j"}


# OpenAI Compatible Configuration - Ollama
#OPENAI_COMPATIBLE_REASONING_MODEL=qwen2.5:latest
#OPENAI_COMPATIBLE_BASE_URL=http://localhost:11434/v1/
#OPENAI_COMPATIBLE_TOKEN=xxxxxx


# MCP Server Authentication Settings (Optional - only for HTTP transport MCP server)
# These settings are for the main MCP server, NOT the OAuth proxy
# Set to 'true' to enable JWT Bearer token authentication for MCP requests
ENABLE_AUTH=false

# Choose ONE of these two options:
# Option A: Static public key (for development/testing)
#AUTH_PUBLIC_KEY=

# Option B: JWKS URI (recommended for production)
AUTH_JWKS_URI=https://your-identity-provider.com/.well-known/jwks.json

# Optional JWT validation settings for MCP server
AUTH_ISSUER=
AUTH_AUDIENCE=okta-mcp-server
AUTH_REQUIRED_SCOPES=